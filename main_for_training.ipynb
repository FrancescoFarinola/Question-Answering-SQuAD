{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219b4988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\francesco.farinola\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from tensorflow import one_hot\n",
    "import load_data\n",
    "import preprocess\n",
    "import utils\n",
    "import drqa_model\n",
    "import bidaf_model\n",
    "import our_model\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "\n",
    "from os.path import isfile\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json \n",
    "from settings import EMBEDDING_DIM, MODEL, EPOCHS, BATCH_SIZE, MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3d4722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "(87599, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "dataframe = load_data.load_dataset()\n",
    "print(dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34761949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train and test set...\n",
      "(78161, 6) (9438, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting train and test set...\")\n",
    "train_df, test_df = load_data.split_test_set(dataframe)\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41bd3ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train and validation set...\n",
      "(61143, 6) (17018, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting train and validation set...\")\n",
    "train_df, val_df = load_data.split_validation_set(train_df, rate=0.2)\n",
    "print(train_df.shape, val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c9dbf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "Preprocessing validation data...\n"
     ]
    }
   ],
   "source": [
    "PREPROCESSING_PIPELINE1 = [preprocess.expand_contractions,\n",
    "                           preprocess.tokenization_spacy,\n",
    "                           preprocess.remove_chars,\n",
    "                           preprocess.split_alpha_num_sym,\n",
    "                           preprocess.spell_correction,\n",
    "                           preprocess.lemmatization,\n",
    "                           preprocess.lower,\n",
    "                           preprocess.strip_text]\n",
    "\n",
    "print(\"Preprocessing training data...\")\n",
    "train_df1 = train_df.copy()\n",
    "train_df1, train_tmp1 = preprocess.apply_preprocessing(train_df1, PREPROCESSING_PIPELINE1)\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "val_df1 = val_df.copy()\n",
    "val_df1, val_tmp1 = preprocess.apply_preprocessing(val_df1, PREPROCESSING_PIPELINE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3c4c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import importlib\n",
    "#importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32502566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load: True\n",
      "Loading matrices, tokenizers and dictionaries... \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# load already saved content or compute it from scratch\n",
    "load = (isfile(f\"{MODELS_DIR}/word_listing.csv\") and \n",
    "        isfile(f\"{MODELS_DIR}/word2idx.json\") and\n",
    "        isfile(f\"{MODELS_DIR}/idx2word.json\") and\n",
    "        isfile(f\"{MODELS_DIR}/tokenizer.json\") and\n",
    "        isfile(f\"{MODELS_DIR}/embedding_matrix.csv\") and\n",
    "        # char embedding matrix is loaded only in case of 'our_model' or 'bidaf'\n",
    "        (not(MODEL == \"our_model\" or MODEL == 'bidaf') or\n",
    "         isfile(f\"{MODELS_DIR}/char_embedding_matrix.csv\")))\n",
    "print(\"load:\", load)\n",
    "\n",
    "if load:\n",
    "    print(\"Loading matrices, tokenizers and dictionaries... \")\n",
    "    #load pre-saved \n",
    "    df_word_listing = np.genfromtxt(f\"{MODELS_DIR}/word_listing.csv\", delimiter=',', encoding='utf-8', dtype='str')\n",
    "    \n",
    "    with open(f\"{MODELS_DIR}/word2idx.json\") as f:\n",
    "        df_word_to_idx = json.load(f)\n",
    "\n",
    "    with open(f\"{MODELS_DIR}/idx2word.json\") as f:\n",
    "        df_idx_to_word = json.load(f)\n",
    "\n",
    "    with open(f\"{MODELS_DIR}/tokenizer.json\") as f:\n",
    "        tokenizer_json = json.load(f)\n",
    "        df_tokenizer = tokenizer_from_json(tokenizer_json)\n",
    "\n",
    "    embedding_matrix = np.genfromtxt(f\"{MODELS_DIR}/embedding_matrix.csv\", delimiter=',')\n",
    "    print(\"Done\")\n",
    "          \n",
    "else:\n",
    "    #compute \n",
    "    print(\"Computing matrices, tokenizers and dictionaries... \")\n",
    "    embedding_matrix, df_word_listing, df_tokenizer, df_word_to_idx, df_idx_to_word = utils.get_embedding_matrix(train_df1, EMBEDDING_DIM)\n",
    "    \n",
    "    np.savetxt(f\"{MODELS_DIR}/embedding_matrix.csv\", embedding_matrix, delimiter=\",\")\n",
    "    np.savetxt(f\"{MODELS_DIR}/word_listing.csv\", df_word_listing, delimiter=\",\", fmt =\"%s\", encoding='utf-8')\n",
    "\n",
    "    with open(f\"{MODELS_DIR}/word2idx.json\", 'w') as f:\n",
    "        json.dump(df_word_to_idx, f)\n",
    "\n",
    "    with open(f\"{MODELS_DIR}/idx2word.json\", 'w') as f:\n",
    "        json.dump(df_idx_to_word, f)\n",
    "\n",
    "    tokenizer_json = df_tokenizer.to_json()\n",
    "    with io.open(f\"{MODELS_DIR}/tokenizer.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b10996e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx_to_word = dict(zip([int(k) for k in df_idx_to_word.keys()], df_idx_to_word.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c009329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length for context is 662\n",
      "Max length adopted for context is 728\n",
      "Max length for answer is 43\n",
      "Max length adopted for answer is 47\n",
      "Max length for question is 40\n",
      "Max length adopted for question is 44\n",
      "Padding data...\n"
     ]
    }
   ],
   "source": [
    "MAX_CONTEXT_LENGTH, MAX_TEXT_LENGTH, MAX_QUESTION_LENGTH = utils.get_max_length(train_df1)\n",
    "\n",
    "#MAX_CONTEXT_LENGTH, MAX_TEXT_LENGTH, MAX_QUESTION_LENGTH = 662, 43, 40\n",
    "print(\"Padding data...\")\n",
    "tr_context_padded = utils.pad(train_df1.context, df_tokenizer, MAX_CONTEXT_LENGTH)\n",
    "tr_answer_padded = utils.pad(train_df1.text, df_tokenizer, MAX_TEXT_LENGTH)\n",
    "tr_question_padded = utils.pad(train_df1.question, df_tokenizer, MAX_QUESTION_LENGTH)\n",
    "\n",
    "val_context_padded = utils.pad(val_df1.context, df_tokenizer, MAX_CONTEXT_LENGTH)\n",
    "val_answer_padded = utils.pad(val_df1.text, df_tokenizer, MAX_TEXT_LENGTH)\n",
    "val_question_padded = utils.pad(val_df1.question, df_tokenizer, MAX_QUESTION_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76a9ec56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing start and end indices... \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing start and end indices... \")\n",
    "train_df1['s_idx'] = train_df.apply(\n",
    "    lambda x: len(preprocess.preprocessing(x.context[:x.answer_start], PREPROCESSING_PIPELINE1).split()), axis=1)\n",
    "train_df1['e_idx'] = train_df1.apply(lambda x: x.s_idx + len(x.text.split()) - 1, axis=1)\n",
    "\n",
    "val_df1['s_idx'] = val_df.apply(\n",
    "    lambda x: len(preprocess.preprocessing(x.context[:x.answer_start], PREPROCESSING_PIPELINE1).split()), axis=1)\n",
    "val_df1['e_idx'] = val_df1.apply(lambda x: x.s_idx + len(x.text.split()) - 1, axis=1)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79570526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'our_model' from 'C:\\\\Users\\\\francesco.farinola\\\\SQuAD\\\\our_model.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = 'our_model'\n",
    "import importlib\n",
    "importlib.reload(our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebbcb3b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionaries for POS tags...\n",
      "Creating dictionaries for NER tags...\n",
      "Extracting features for Train Set\n",
      "Computing original exact match...\n",
      "Computing lowercase exact match...\n",
      "Computing lemmatized exact match...\n",
      "Computing TF...\n",
      "Computing POS tags...\n",
      "Padding POS sequences...\n",
      "Computing NER tags...\n",
      "Padding NER sequences...\n",
      "Extracting features for Validation Set\n",
      "Computing original exact match...\n",
      "Computing lowercase exact match...\n",
      "Computing lemmatized exact match...\n",
      "Computing TF...\n",
      "Computing POS tags...\n",
      "Padding POS sequences...\n",
      "Computing NER tags...\n",
      "Padding NER sequences...\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "context (InputLayer)            [(None, 728)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question (InputLayer)           [(None, 44)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "p_encoding (Embedding)          (None, 728, 200)     13048800    context[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "char_p_encoding (Embedding)     (None, 728, 50)      3262200     context[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q_encoding (Embedding)          (None, 44, 200)      13048800    question[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "char_q_encoding (Embedding)     (None, 44, 50)       3262200     question[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat_p (Concatenate)          (None, 728, 250)     0           p_encoding[0][0]                 \n",
      "                                                                 char_p_encoding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concat_q (Concatenate)          (None, 44, 250)      0           q_encoding[0][0]                 \n",
      "                                                                 char_q_encoding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_p (Dense)                 (None, 728, 100)     25100       concat_p[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_q (Dense)                 (None, 44, 100)      25100       concat_q[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "biH (Bidirectional)             (None, 728, 100)     45600       dense_p[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "biU (Bidirectional)             (None, 44, 100)      45600       dense_q[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "S (SimilarityLayer)             (None, 728, 44)      300         biH[0][0]                        \n",
      "                                                                 biU[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pos (InputLayer)                [(None, 728)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ner (InputLayer)                [(None, 728)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "C2Q (C2Q)                       (None, 728, 100)     0           biU[0][0]                        \n",
      "                                                                 S[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "Q2C (Q2C)                       (None, 728, 100)     0           biH[0][0]                        \n",
      "                                                                 S[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "em (InputLayer)                 [(None, 728, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_encoding (Embedding)        (None, 728, 54)      2916        pos[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "ner_encoding (Embedding)        (None, 728, 20)      400         ner[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf (InputLayer)                 [(None, 728, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "G (MergeG)                      (None, 728, 478)     0           biH[0][0]                        \n",
      "                                                                 C2Q[0][0]                        \n",
      "                                                                 Q2C[0][0]                        \n",
      "                                                                 em[0][0]                         \n",
      "                                                                 pos_encoding[0][0]               \n",
      "                                                                 ner_encoding[0][0]               \n",
      "                                                                 tf[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "M (Bidirectional)               (None, 728, 100)     159000      G[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "M2 (Bidirectional)              (None, 728, 100)     45600       M[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "GM (Concatenate)                (None, 728, 578)     0           G[0][0]                          \n",
      "                                                                 M[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "GM2 (Concatenate)               (None, 728, 578)     0           G[0][0]                          \n",
      "                                                                 M2[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "td_s (TimeDistributed)          (None, 728, 1)       579         GM[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "td_e (TimeDistributed)          (None, 728, 1)       579         GM2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze (TFOpLambd (None, 728)          0           td_s[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze_1 (TFOpLam (None, 728)          0           td_e[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "start_ (Softmax)                (None, 728)          0           tf.compat.v1.squeeze[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "end_ (Softmax)                  (None, 728)          0           tf.compat.v1.squeeze_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "prediction (Prediction)         (None, 728, 728)     0           start_[0][0]                     \n",
      "                                                                 end_[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "start (Lambda)                  (None, 728)          0           prediction[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "end (Lambda)                    (None, 728)          0           prediction[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 32,972,774\n",
      "Trainable params: 347,458\n",
      "Non-trainable params: 32,625,316\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#save weights instead of saving entire model\n",
    "\n",
    "if MODEL == 'basemodel' or MODEL == None:\n",
    "    pass\n",
    "\n",
    "elif MODEL == 'drqa':\n",
    "    tag2idx, idx2tag = utils.create_pos_dicts()\n",
    "    ner2idx, idx2ner = utils.create_ner_dicts()\n",
    "\n",
    "    pos_embedding_matrix = to_categorical(list(idx2tag.keys()))\n",
    "    ner_embedding_matrix = to_categorical(list(idx2ner.keys()))\n",
    "\n",
    "    print(\"Extracting features for Train Set\")\n",
    "    train_em_input = utils.compute_exact_match(train_df1, MAX_CONTEXT_LENGTH)\n",
    "    train_tf_input = utils.compute_tf(train_df1, MAX_CONTEXT_LENGTH)\n",
    "    train_pos_input = utils.compute_pos(train_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    train_ner_input = utils.compute_ner(train_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    print(\"Extracting features for Validation Set\")\n",
    "    val_em_input = utils.compute_exact_match(val_df1, MAX_CONTEXT_LENGTH)\n",
    "    val_tf_input = utils.compute_tf(val_df1, MAX_CONTEXT_LENGTH)\n",
    "    val_pos_input = utils.compute_pos(val_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    val_ner_input = utils.compute_ner(val_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "\n",
    "    model = drqa_model.build_model(MAX_QUESTION_LENGTH, MAX_CONTEXT_LENGTH, EMBEDDING_DIM, embedding_matrix,\n",
    "                pos_embedding_matrix, ner_embedding_matrix)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics='accuracy')\n",
    "    model.summary()\n",
    "    plot_model(model, rankdir='TB', show_shapes=True, show_dtype=True, to_file=f\"{MODELS_DIR}/drqa.png\")\n",
    "\n",
    "    tr_s_one = one_hot(train_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    tr_e_one = one_hot(train_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_s_one = one_hot(val_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_e_one = one_hot(val_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    x_tr = {'context': tr_context_padded, 'question': tr_question_padded, 'pos': train_pos_input,\n",
    "            'ner': train_ner_input, 'em': train_em_input, 'tf': train_tf_input}\n",
    "    x_val = {'context': val_context_padded, 'question': val_question_padded, 'pos': val_pos_input,\n",
    "             'ner': val_ner_input, 'em': val_em_input, 'tf': val_tf_input}\n",
    "\n",
    "    y_tr = {'start': tr_s_one, 'end': tr_e_one}\n",
    "    y_val = {'start': val_s_one, 'end': val_e_one}\n",
    "\n",
    "    #mycb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    #model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[mycb])\n",
    "    #model.save_weights(f\"{MODELS_DIR}/drqa_weights.h5\")\n",
    "\n",
    "elif MODEL == \"bidaf\":\n",
    "\n",
    "    char_embedding_matrix = utils.get_char_embeddings(df_word_listing, df_word_to_idx)\n",
    "\n",
    "    model = bidaf_model.build_model(MAX_QUESTION_LENGTH, MAX_CONTEXT_LENGTH, EMBEDDING_DIM, embedding_matrix, char_embedding_matrix)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics='accuracy')\n",
    "    model.summary()\n",
    "    plot_model(model, rankdir='TB', show_shapes=True, show_dtype=True, to_file=f\"{MODELS_DIR}/bidaf.png\")\n",
    "\n",
    "    tr_s_one = one_hot(train_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    tr_e_one = one_hot(train_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_s_one = one_hot(val_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_e_one = one_hot(val_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    x_tr = {'context': tr_context_padded, 'question': tr_question_padded}\n",
    "    x_val = {'context': val_context_padded, 'question': val_question_padded}\n",
    "\n",
    "    y_tr = {'start': tr_s_one, 'end': tr_e_one}\n",
    "    y_val = {'start': val_s_one, 'end': val_e_one}\n",
    "\n",
    "    #mycb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    #model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[mycb])\n",
    "    #model.save(f\"{MODELS_DIR}/bidaf_weights.h5\")\n",
    "\n",
    "elif MODEL == \"our_model\":\n",
    "    tag2idx, idx2tag = utils.create_pos_dicts()\n",
    "    ner2idx, idx2ner = utils.create_ner_dicts()\n",
    "\n",
    "    pos_embedding_matrix = to_categorical(list(idx2tag.keys()))\n",
    "    ner_embedding_matrix = to_categorical(list(idx2ner.keys()))\n",
    "\n",
    "    print(\"Extracting features for Train Set\")\n",
    "    train_em_input = utils.compute_exact_match(train_df1, MAX_CONTEXT_LENGTH)\n",
    "    train_tf_input = utils.compute_tf(train_df1, MAX_CONTEXT_LENGTH)\n",
    "    train_pos_input = utils.compute_pos(train_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    train_ner_input = utils.compute_ner(train_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    print(\"Extracting features for Validation Set\")\n",
    "    val_em_input = utils.compute_exact_match(val_df1, MAX_CONTEXT_LENGTH)\n",
    "    val_tf_input = utils.compute_tf(val_df1, MAX_CONTEXT_LENGTH)\n",
    "    val_pos_input = utils.compute_pos(val_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    val_ner_input = utils.compute_ner(val_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    if isfile(f\"{MODELS_DIR}/char_embedding_matrix.csv\"):\n",
    "        char_embedding_matrix = np.genfromtxt(f\"{MODELS_DIR}/char_embedding_matrix.csv\", delimiter=',')\n",
    "    else:\n",
    "        char_embedding_matrix = utils.get_char_embeddings(df_word_listing, df_word_to_idx)\n",
    "        np.savetxt(f\"{MODELS_DIR}/char_embedding_matrix.csv\", char_embedding_matrix, delimiter=\",\")\n",
    "\n",
    "    model = our_model.build_model(MAX_QUESTION_LENGTH, MAX_CONTEXT_LENGTH, EMBEDDING_DIM,\n",
    "                                  embedding_matrix, char_embedding_matrix, pos_embedding_matrix, ner_embedding_matrix)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics='accuracy')\n",
    "    model.summary()\n",
    "    plot_model(model, rankdir='TB', show_shapes=True, show_dtype=True, to_file=f\"{MODELS_DIR}/our_model.png\")\n",
    "\n",
    "    tr_s_one = one_hot(train_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    tr_e_one = one_hot(train_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_s_one = one_hot(val_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_e_one = one_hot(val_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    x_tr = {'context': tr_context_padded, 'question': tr_question_padded, 'pos': train_pos_input,\n",
    "            'ner': train_ner_input, 'em': train_em_input, 'tf': train_tf_input}\n",
    "    x_val = {'context': val_context_padded, 'question': val_question_padded, 'pos': val_pos_input,\n",
    "             'ner': val_ner_input, 'em': val_em_input, 'tf': val_tf_input}\n",
    "\n",
    "    y_tr = {'start': tr_s_one, 'end': tr_e_one}\n",
    "    y_val = {'start': val_s_one, 'end': val_e_one}\n",
    "\n",
    "    #mycb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    #model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[mycb])\n",
    "    #model.save_weights(f\"{MODELS_DIR}/our_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfe2bca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12038"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc45055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3822/3822 [==============================] - 1288s 333ms/step - loss: 5.5451 - start_loss: 2.8808 - end_loss: 2.6643 - start_accuracy: 0.3041 - end_accuracy: 0.3220 - val_loss: 4.5044 - val_start_loss: 2.3570 - val_end_loss: 2.1474 - val_start_accuracy: 0.3854 - val_end_accuracy: 0.4257\n"
     ]
    }
   ],
   "source": [
    "mycb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=1, batch_size=16, callbacks=[mycb])\n",
    "model.save_weights(f\"{MODELS_DIR}/our_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf800cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"{MODELS_DIR}/our_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e382cea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_4_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/our_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/our_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(f\"{MODELS_DIR}/our_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6379c93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing original exact match...\n",
      "Computing lowercase exact match...\n",
      "Computing lemmatized exact match...\n",
      "Computing TF...\n",
      "Computing POS tags...\n",
      "Padding POS sequences...\n",
      "Computing NER tags...\n",
      "Padding NER sequences...\n"
     ]
    }
   ],
   "source": [
    "test_df1 = test_df.copy()\n",
    "test_df1, test_tmp1 = preprocess.apply_preprocessing(test_df1, PREPROCESSING_PIPELINE1)\n",
    "\n",
    "ts_context_padded = utils.pad(test_df1.context, df_tokenizer, MAX_CONTEXT_LENGTH)\n",
    "ts_answer_padded = utils.pad(test_df1.text, df_tokenizer, MAX_TEXT_LENGTH)\n",
    "ts_question_padded = utils.pad(test_df1.question, df_tokenizer, MAX_QUESTION_LENGTH)\n",
    "\n",
    "test_df1['s_idx'] = test_df.apply(\n",
    "    lambda x: len(preprocess.preprocessing(x.context[:x.answer_start], PREPROCESSING_PIPELINE1).split()), axis=1)\n",
    "test_df1['e_idx'] = test_df1.apply(lambda x: x.s_idx + len(x.text.split()) - 1, axis=1)\n",
    "\n",
    "ts_s_one = one_hot(test_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "ts_e_one = one_hot(test_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "if MODEL == 'drqa' or MODEL == \"our_model\":\n",
    "\n",
    "    ts_em_input = utils.compute_exact_match(test_df1, MAX_CONTEXT_LENGTH)\n",
    "    ts_tf_input = utils.compute_tf(test_df1, MAX_CONTEXT_LENGTH)\n",
    "    ts_pos_input = utils.compute_pos(test_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    ts_ner_input = utils.compute_ner(test_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    x_ts = {'context': ts_context_padded, 'question': ts_question_padded, 'pos': ts_pos_input,\n",
    "            'ner': ts_ner_input, 'em': ts_em_input, 'tf': ts_tf_input}\n",
    "    y_ts = {'start': ts_s_one, 'end': ts_e_one}\n",
    "else:\n",
    "    x_ts = {'context': ts_context_padded, 'question': ts_question_padded}\n",
    "    y_ts = {'start': ts_s_one, 'end': ts_e_one}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "220a2a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutating model...\n",
      "590/590 [==============================] - 80s 136ms/step - loss: 5.1133 - start_loss: 2.6338 - end_loss: 2.4795 - start_accuracy: 0.3573 - end_accuracy: 0.3762\n",
      "[5.113343238830566, 2.6338155269622803, 2.4795243740081787, 0.35727909207344055, 0.3762449622154236]\n",
      "Preprocessing on datasets...\n",
      "Applying expand_contractions2, tokenization_spacy, remove_chars, split_alpha_num_sym and strip_text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-3a0e9b50b637>:6: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  df = pd.concat([train_df, val_df, test_df], 0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating predictions...\n",
      "Computing answers...\n",
      "Saving predictions as json...\n",
      "Computing F1 score, precision and recall...\n",
      "F1: 0.46439017452659265\t Precision: 0.4952241599725147\t Recall: 0.4976532981226597\t\n"
     ]
    }
   ],
   "source": [
    "print(\"Evalutating model...\")\n",
    "evaluation = model.evaluate(x_ts, y_ts, batch_size=BATCH_SIZE)\n",
    "print(evaluation)\n",
    "\n",
    "\n",
    "df = pd.concat([train_df, val_df, test_df], 0, ignore_index=True)\n",
    "x = {**x_tr, **x_val, **x_ts}\n",
    "predictions = utils.computing_predictions(model, df, x, BATCH_SIZE)\n",
    "\n",
    "#predictions = utils.computing_predictions(model, train_df, val_df, test_df, x_tr, x_val, x_ts)\n",
    "\n",
    "print(\"Saving predictions as json...\")\n",
    "with open('predictions.json', 'w') as outfile:\n",
    "    json.dump(predictions, outfile)\n",
    "\n",
    "f1, precision, recall = utils.evaluate_model(model, MAX_CONTEXT_LENGTH, val_df1, x_val)\n",
    "print(f\"F1: {f1}\\t Precision: {precision}\\t Recall: {recall}\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d69bd2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "new_model = our_model.build_model(MAX_QUESTION_LENGTH, MAX_CONTEXT_LENGTH, EMBEDDING_DIM,\n",
    "                                  embedding_matrix, char_embedding_matrix, pos_embedding_matrix, ner_embedding_matrix)\n",
    "new_model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics='accuracy')\n",
    "new_model.load_weights(\"./models/our_weights.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dce937d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutating model...\n",
      "590/590 [==============================] - 90s 136ms/step - loss: 5.1133 - start_loss: 2.6338 - end_loss: 2.4795 - start_accuracy: 0.3573 - end_accuracy: 0.37626s - loss: 5.1313 - start_loss: 2.6418 - end_loss: 2.4895\n",
      "[5.113343238830566, 2.6338155269622803, 2.4795243740081787, 0.35727909207344055, 0.3762449622154236]\n",
      "Preprocessing on datasets...\n",
      "Applying expand_contractions2, tokenization_spacy, remove_chars, split_alpha_num_sym and strip_text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-86025d7f37a0>:6: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  df = pd.concat([train_df, val_df, test_df], 0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating predictions...\n",
      "Computing answers...\n",
      "Computing F1 score, precision and recall...\n",
      "F1: 0.46439017452659265\t Precision: 0.4952241599725147\t Recall: 0.4976532981226597\t\n"
     ]
    }
   ],
   "source": [
    "print(\"Evalutating model...\")\n",
    "evaluation = new_model.evaluate(x_ts, y_ts, batch_size=BATCH_SIZE)\n",
    "print(evaluation)\n",
    "\n",
    "\n",
    "df = pd.concat([train_df, val_df, test_df], 0, ignore_index=True)\n",
    "x = {**x_tr, **x_val, **x_ts}\n",
    "predictions = utils.computing_predictions(new_model, df, x, BATCH_SIZE)\n",
    "\n",
    "f1, precision, recall = utils.evaluate_model(new_model, MAX_CONTEXT_LENGTH, val_df1, x_val)\n",
    "print(f\"F1: {f1}\\t Precision: {precision}\\t Recall: {recall}\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4035e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./models/bidaf.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18a731f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_fn, lstm_cell_12_layer_call_and_return_conditional_losses, lstm_cell_12_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/bidaf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/bidaf\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./models/bidaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c46dfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
