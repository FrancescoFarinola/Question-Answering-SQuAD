{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219b4988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\userl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from tensorflow import one_hot\n",
    "import load_data\n",
    "import preprocess\n",
    "import utils\n",
    "import drqa_model\n",
    "import bidaf_model\n",
    "import our_model\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "\n",
    "from os.path import isfile\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json \n",
    "from settings import EMBEDDING_DIM, MODEL, EPOCHS, BATCH_SIZE, MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0b884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMBEDDING_DIM = settings.EMBEDDING_DIM\n",
    "#MODEL = settings.MODEL\n",
    "#EPOCHS = settings.EPOCHS\n",
    "#BATCH_SIZE = settings.BATCH_SIZE\n",
    "#models_dir = settings.MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d3d4722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "(5000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "dataframe = load_data.load_dataset()\n",
    "dataframe = dataframe[:5000]\n",
    "print(dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34761949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train and test set...\n",
      "(4435, 6) (577, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting train and test set...\")\n",
    "train_df, test_df = load_data.split_test_set(dataframe)\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bd3ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train and validation set...\n",
      "(3937, 6) (498, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting train and validation set...\")\n",
    "train_df, val_df = load_data.split_validation_set(train_df, rate=0.2)\n",
    "print(train_df.shape, val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c9dbf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "Preprocessing validation data...\n"
     ]
    }
   ],
   "source": [
    "PREPROCESSING_PIPELINE1 = [preprocess.expand_contractions,\n",
    "                           preprocess.tokenization_spacy,\n",
    "                           preprocess.remove_chars,\n",
    "                           preprocess.split_alpha_num_sym,\n",
    "                           preprocess.spell_correction,\n",
    "                           preprocess.lemmatization,\n",
    "                           preprocess.lower,\n",
    "                           preprocess.strip_text]\n",
    "\n",
    "print(\"Preprocessing training data...\")\n",
    "train_df1 = train_df.copy()\n",
    "train_df1, train_tmp1 = preprocess.apply_preprocessing(train_df1, PREPROCESSING_PIPELINE1)\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "val_df1 = val_df.copy()\n",
    "val_df1, val_tmp1 = preprocess.apply_preprocessing(val_df1, PREPROCESSING_PIPELINE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c53dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the already saved content or compute from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32502566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load: True\n",
      "Loading matrices, tokenizers and dictionaries... \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# load already saved content or compute it from scratch\n",
    "load = (isfile(f\"{MODELS_DIR}/word_listing.csv\") and \n",
    "        isfile(f\"{MODELS_DIR}/word2idx.json\") and\n",
    "        isfile(f\"{MODELS_DIR}/idx2word.json\") and\n",
    "        isfile(f\"{MODELS_DIR}/tokenizer.json\") and\n",
    "        isfile(f\"{MODELS_DIR}/embedding_matrix.csv\"))\n",
    "print(\"load:\", load)\n",
    "\n",
    "if load:\n",
    "    print(\"Loading matrices, tokenizers and dictionaries... \")\n",
    "    #load pre-saved \n",
    "    df_word_listing = np.genfromtxt(f\"{MODELS_DIR}/word_listing.csv\", delimiter=',', encoding='utf-8', dtype='str')\n",
    "    \n",
    "    with open(f\"{MODELS_DIR}/word2idx.json\") as f:\n",
    "        df_word_to_idx = json.load(f)\n",
    "\n",
    "    with open(f\"{MODELS_DIR}/idx2word.json\") as f:\n",
    "        df_idx_to_word = json.load(f)\n",
    "\n",
    "    with open(f\"{MODELS_DIR}/tokenizer.json\") as f:\n",
    "        tokenizer_json = json.load(f)\n",
    "        df_tokenizer = tokenizer_from_json(tokenizer_json)\n",
    "\n",
    "    embedding_matrix = np.genfromtxt(f\"{models_dir}/embedding_matrix.csv\", delimiter=',')\n",
    "    print(\"Done\")\n",
    "          \n",
    "else:\n",
    "    #compute \n",
    "    print(\"Computing matrices, tokenizers and dictionaries... \")\n",
    "    embedding_matrix, df_word_listing, df_tokenizer, df_word_to_idx, df_idx_to_word = utils.get_embedding_matrix(train_df1, EMBEDDING_DIM)\n",
    "    \n",
    "    np.savetxt(f\"{MODELS_DIR}/embedding_matrix.csv\", embedding_matrix, delimiter=\",\")\n",
    "    np.savetxt(f\"{MODELS_DIR}/word_listing.csv\", df_word_listing, delimiter=\",\", fmt =\"%s\", encoding='utf-8')\n",
    "\n",
    "    with open(f\"{MODELS_DIR}/word2idx.json\", 'w') as f:\n",
    "        json.dump(df_word_to_idx, f)\n",
    "\n",
    "    with open(f\"{MODELS_DIR}/idx2word.json\", 'w') as f:\n",
    "        json.dump(df_idx_to_word, f)\n",
    "\n",
    "    tokenizer_json = df_tokenizer.to_json()\n",
    "    with io.open(f\"{MODELS_DIR}/tokenizer.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b10996e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx_to_word = dict(zip([int(k) for k in df_idx_to_word.keys()], df_idx_to_word.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c009329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length for context is 518\n",
      "Max length adopted for context is 569\n",
      "Max length for answer is 43\n",
      "Max length adopted for answer is 47\n",
      "Max length for question is 30\n",
      "Max length adopted for question is 33\n",
      "Padding data...\n"
     ]
    }
   ],
   "source": [
    "MAX_CONTEXT_LENGTH, MAX_TEXT_LENGTH, MAX_QUESTION_LENGTH = utils.get_max_length(train_df1)\n",
    "\n",
    "print(\"Padding data...\")\n",
    "tr_context_padded = utils.pad(train_df1.context, df_tokenizer, MAX_CONTEXT_LENGTH)\n",
    "tr_answer_padded = utils.pad(train_df1.text, df_tokenizer, MAX_TEXT_LENGTH)\n",
    "tr_question_padded = utils.pad(train_df1.question, df_tokenizer, MAX_QUESTION_LENGTH)\n",
    "\n",
    "val_context_padded = utils.pad(val_df1.context, df_tokenizer, MAX_CONTEXT_LENGTH)\n",
    "val_answer_padded = utils.pad(val_df1.text, df_tokenizer, MAX_TEXT_LENGTH)\n",
    "val_question_padded = utils.pad(val_df1.question, df_tokenizer, MAX_QUESTION_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76a9ec56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing start and end indices... \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing start and end indices... \")\n",
    "train_df1['s_idx'] = train_df.apply(\n",
    "    lambda x: len(preprocess.preprocessing(x.context[:x.answer_start], PREPROCESSING_PIPELINE1).split()), axis=1)\n",
    "train_df1['e_idx'] = train_df1.apply(lambda x: x.s_idx + len(x.text.split()) - 1, axis=1)\n",
    "\n",
    "val_df1['s_idx'] = val_df.apply(\n",
    "    lambda x: len(preprocess.preprocessing(x.context[:x.answer_start], PREPROCESSING_PIPELINE1).split()), axis=1)\n",
    "val_df1['e_idx'] = val_df1.apply(lambda x: x.s_idx + len(x.text.split()) - 1, axis=1)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7bf7012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionaries for POS tags...\n",
      "Creating dictionaries for NER tags...\n",
      "Extracting features for Train Set\n",
      "Computing original exact match...\n",
      "Computing lowercase exact match...\n",
      "Computing lemmatized exact match...\n",
      "Computing normalized TF...\n",
      "Computing POS tags...\n",
      "Padding POS sequences...\n",
      "Computing NER tags...\n",
      "Padding NER sequences...\n",
      "Extracting features for Validation Set\n",
      "Computing original exact match...\n",
      "Computing lowercase exact match...\n",
      "Computing lemmatized exact match...\n",
      "Computing normalized TF...\n",
      "Computing POS tags...\n",
      "Padding POS sequences...\n",
      "Computing NER tags...\n",
      "Padding NER sequences...\n",
      "Computing character-level embeddings...\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "context (InputLayer)            [(None, 569)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question (InputLayer)           [(None, 33)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "p_encoding (Embedding)          (None, 569, 100)     1094800     context[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "char_p_encoding (Embedding)     (None, 569, 50)      547400      context[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q_encoding (Embedding)          (None, 33, 100)      1094800     question[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "char_q_encoding (Embedding)     (None, 33, 50)       547400      question[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat_p (Concatenate)          (None, 569, 150)     0           p_encoding[0][0]                 \n",
      "                                                                 char_p_encoding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concat_q (Concatenate)          (None, 33, 150)      0           q_encoding[0][0]                 \n",
      "                                                                 char_q_encoding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_p (Dense)                 (None, 569, 100)     15100       concat_p[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_q (Dense)                 (None, 33, 100)      15100       concat_q[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "biH (Bidirectional)             (None, 569, 100)     45600       dense_p[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "biU (Bidirectional)             (None, 33, 100)      45600       dense_q[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "S (SimilarityLayer)             (None, 569, 33)      300         biH[0][0]                        \n",
      "                                                                 biU[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pos (InputLayer)                [(None, 569)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ner (InputLayer)                [(None, 569)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "C2Q (C2Q)                       (None, 569, 100)     0           biU[0][0]                        \n",
      "                                                                 S[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "Q2C (Q2C)                       (None, 569, 100)     0           biH[0][0]                        \n",
      "                                                                 S[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "em (InputLayer)                 [(None, 569, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_encoding (Embedding)        (None, 569, 54)      2916        pos[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "ner_encoding (Embedding)        (None, 569, 20)      400         ner[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf (InputLayer)                 [(None, 569, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "G (MergeG)                      (None, 569, 478)     0           biH[0][0]                        \n",
      "                                                                 C2Q[0][0]                        \n",
      "                                                                 Q2C[0][0]                        \n",
      "                                                                 em[0][0]                         \n",
      "                                                                 pos_encoding[0][0]               \n",
      "                                                                 ner_encoding[0][0]               \n",
      "                                                                 tf[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "M (Bidirectional)               (None, 569, 100)     159000      G[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "M2 (Bidirectional)              (None, 569, 100)     45600       M[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "GM (Concatenate)                (None, 569, 578)     0           G[0][0]                          \n",
      "                                                                 M[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "GM2 (Concatenate)               (None, 569, 578)     0           G[0][0]                          \n",
      "                                                                 M2[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "td_s (TimeDistributed)          (None, 569, 1)       579         GM[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "td_e (TimeDistributed)          (None, 569, 1)       579         GM2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze (TFOpLambd (None, 569)          0           td_s[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze_1 (TFOpLam (None, 569)          0           td_e[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "start_ (Softmax)                (None, 569)          0           tf.compat.v1.squeeze[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "end_ (Softmax)                  (None, 569)          0           tf.compat.v1.squeeze_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "prediction (Prediction)         (None, 569, 569)     0           start_[0][0]                     \n",
      "                                                                 end_[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "start (Lambda)                  (None, 569)          0           prediction[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "end (Lambda)                    (None, 569)          0           prediction[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,615,174\n",
      "Trainable params: 327,458\n",
      "Non-trainable params: 3,287,716\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493/493 [==============================] - 83s 137ms/step - loss: 7.1103 - start_loss: 3.5770 - end_loss: 3.5333 - start_accuracy: 0.1760 - end_accuracy: 0.1707 - val_loss: 7.9030 - val_start_loss: 3.8131 - val_end_loss: 4.0899 - val_start_accuracy: 0.1084 - val_end_accuracy: 0.1084\n"
     ]
    }
   ],
   "source": [
    "#save weights instead of saving entire model\n",
    "\n",
    "if MODEL == 'basemodel' or MODEL == None:\n",
    "    pass\n",
    "\n",
    "elif MODEL == 'drqa':\n",
    "    tag2idx, idx2tag = utils.create_pos_dicts()\n",
    "    ner2idx, idx2ner = utils.create_ner_dicts()\n",
    "\n",
    "    pos_embedding_matrix = to_categorical(list(idx2tag.keys()))\n",
    "    ner_embedding_matrix = to_categorical(list(idx2ner.keys()))\n",
    "\n",
    "    print(\"Extracting features for Train Set\")\n",
    "    train_em_input = utils.compute_exact_match(train_df1, MAX_CONTEXT_LENGTH)\n",
    "    train_tf_input = utils.compute_tf(train_df1, MAX_CONTEXT_LENGTH)\n",
    "    train_pos_input = utils.compute_pos(train_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    train_ner_input = utils.compute_ner(train_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    print(\"Extracting features for Validation Set\")\n",
    "    val_em_input = utils.compute_exact_match(val_df1, MAX_CONTEXT_LENGTH)\n",
    "    val_tf_input = utils.compute_tf(val_df1, MAX_CONTEXT_LENGTH)\n",
    "    val_pos_input = utils.compute_pos(val_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    val_ner_input = utils.compute_ner(val_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "\n",
    "    model = drqa_model.build_model(MAX_QUESTION_LENGTH, MAX_CONTEXT_LENGTH, EMBEDDING_DIM, embedding_matrix,\n",
    "                pos_embedding_matrix, ner_embedding_matrix)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics='accuracy')\n",
    "    model.summary()\n",
    "    plot_model(model, rankdir='TB', show_shapes=True, show_dtype=True, to_file=\"/models/drqa.png\")\n",
    "\n",
    "    tr_s_one = one_hot(train_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    tr_e_one = one_hot(train_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_s_one = one_hot(val_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_e_one = one_hot(val_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    x_tr = {'context': tr_context_padded, 'question': tr_question_padded, 'pos': train_pos_input,\n",
    "            'ner': train_ner_input, 'em': train_em_input, 'tf': train_tf_input}\n",
    "    x_val = {'context': val_context_padded, 'question': val_question_padded, 'pos': val_pos_input,\n",
    "             'ner': val_ner_input, 'em': val_em_input, 'tf': val_tf_input}\n",
    "\n",
    "    y_tr = {'start': tr_s_one, 'end': tr_e_one}\n",
    "    y_val = {'start': val_s_one, 'end': val_e_one}\n",
    "\n",
    "    mycb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=EPOCHS, batch_size=16, callbacks=[mycb])\n",
    "    model.save_weights(f\"{MODELS_DIR}/drqa_weights.h5\")\n",
    "\n",
    "elif MODEL == \"bidaf\":\n",
    "\n",
    "    char_embedding_matrix = utils.get_char_embeddings(df_word_listing, df_word_to_idx)\n",
    "\n",
    "    model = bidaf_model.build_model(MAX_QUESTION_LENGTH, MAX_CONTEXT_LENGTH, EMBEDDING_DIM, embedding_matrix, char_embedding_matrix)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics='accuracy')\n",
    "    model.summary()\n",
    "    plot_model(model, rankdir='TB', show_shapes=True, show_dtype=True, to_file=\"./models/bidaf.png\")\n",
    "\n",
    "    tr_s_one = one_hot(train_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    tr_e_one = one_hot(train_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_s_one = one_hot(val_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_e_one = one_hot(val_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    x_tr = {'context': tr_context_padded, 'question': tr_question_padded}\n",
    "    x_val = {'context': val_context_padded, 'question': val_question_padded}\n",
    "\n",
    "    y_tr = {'start': tr_s_one, 'end': tr_e_one}\n",
    "    y_val = {'start': val_s_one, 'end': val_e_one}\n",
    "\n",
    "    mycb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=EPOCHS, batch_size=16, callbacks=[mycb])\n",
    "    model.save(f\"{MODELS_DIR}/bidaf_weights.h5\")\n",
    "\n",
    "elif MODEL == \"our_model\":\n",
    "    tag2idx, idx2tag = utils.create_pos_dicts()\n",
    "    ner2idx, idx2ner = utils.create_ner_dicts()\n",
    "\n",
    "    pos_embedding_matrix = to_categorical(list(idx2tag.keys()))\n",
    "    ner_embedding_matrix = to_categorical(list(idx2ner.keys()))\n",
    "\n",
    "    print(\"Extracting features for Train Set\")\n",
    "    train_em_input = utils.compute_exact_match(train_df1, MAX_CONTEXT_LENGTH)\n",
    "    train_tf_input = utils.compute_tf(train_df1, MAX_CONTEXT_LENGTH)\n",
    "    train_pos_input = utils.compute_pos(train_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    train_ner_input = utils.compute_ner(train_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    print(\"Extracting features for Validation Set\")\n",
    "    val_em_input = utils.compute_exact_match(val_df1, MAX_CONTEXT_LENGTH)\n",
    "    val_tf_input = utils.compute_tf(val_df1, MAX_CONTEXT_LENGTH)\n",
    "    val_pos_input = utils.compute_pos(val_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    val_ner_input = utils.compute_ner(val_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    if isfile(f\"{MODELS_DIR}/char_embedding_matrix.csv\"):\n",
    "        char_embedding_matrix = np.genfromtxt(f\"{MODELS_DIR}/char_embedding_matrix.csv\", delimiter=',')\n",
    "    else:\n",
    "        char_embedding_matrix = utils.get_char_embeddings(df_word_listing, df_word_to_idx)\n",
    "        np.savetxt(f\"{MODELS_DIR}/char_embedding_matrix.csv\", char_embedding_matrix, delimiter=\",\")\n",
    "\n",
    "    model = our_model.build_model(MAX_QUESTION_LENGTH, MAX_CONTEXT_LENGTH, EMBEDDING_DIM,\n",
    "                                  embedding_matrix, char_embedding_matrix, pos_embedding_matrix, ner_embedding_matrix)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics='accuracy')\n",
    "    model.summary()\n",
    "    plot_model(model, rankdir='TB', show_shapes=True, show_dtype=True, to_file=\"./models/our_model.png\")\n",
    "\n",
    "    tr_s_one = one_hot(train_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    tr_e_one = one_hot(train_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_s_one = one_hot(val_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_e_one = one_hot(val_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    x_tr = {'context': tr_context_padded, 'question': tr_question_padded, 'pos': train_pos_input,\n",
    "            'ner': train_ner_input, 'em': train_em_input, 'tf': train_tf_input}\n",
    "    x_val = {'context': val_context_padded, 'question': val_question_padded, 'pos': val_pos_input,\n",
    "             'ner': val_ner_input, 'em': val_em_input, 'tf': val_tf_input}\n",
    "\n",
    "    y_tr = {'start': tr_s_one, 'end': tr_e_one}\n",
    "    y_val = {'start': val_s_one, 'end': val_e_one}\n",
    "\n",
    "    mycb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[mycb])\n",
    "    model.save_weights(f\"{MODELS_DIR}/our_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6379c93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing original exact match...\n",
      "Computing lowercase exact match...\n",
      "Computing lemmatized exact match...\n",
      "Computing normalized TF...\n",
      "Computing POS tags...\n",
      "Padding POS sequences...\n",
      "Computing NER tags...\n",
      "Padding NER sequences...\n"
     ]
    }
   ],
   "source": [
    "test_df1 = test_df.copy()\n",
    "test_df1, test_tmp1 = preprocess.apply_preprocessing(test_df1, PREPROCESSING_PIPELINE1)\n",
    "\n",
    "ts_context_padded = utils.pad(test_df1.context, df_tokenizer, MAX_CONTEXT_LENGTH)\n",
    "ts_answer_padded = utils.pad(test_df1.text, df_tokenizer, MAX_TEXT_LENGTH)\n",
    "ts_question_padded = utils.pad(test_df1.question, df_tokenizer, MAX_QUESTION_LENGTH)\n",
    "\n",
    "test_df1['s_idx'] = test_df.apply(\n",
    "    lambda x: len(preprocess.preprocessing(x.context[:x.answer_start], PREPROCESSING_PIPELINE1).split()), axis=1)\n",
    "test_df1['e_idx'] = test_df1.apply(lambda x: x.s_idx + len(x.text.split()) - 1, axis=1)\n",
    "\n",
    "ts_s_one = one_hot(test_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "ts_e_one = one_hot(test_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "if MODEL == 'drqa' or MODEL == \"our_model\":\n",
    "\n",
    "    ts_em_input = utils.compute_exact_match(test_df1, MAX_CONTEXT_LENGTH)\n",
    "    ts_tf_input = utils.compute_tf(test_df1, MAX_CONTEXT_LENGTH)\n",
    "    ts_pos_input = utils.compute_pos(test_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    ts_ner_input = utils.compute_ner(test_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    x_ts = {'context': ts_context_padded, 'question': ts_question_padded, 'pos': ts_pos_input,\n",
    "            'ner': ts_ner_input, 'em': ts_em_input, 'tf': ts_tf_input}\n",
    "    y_ts = {'start': ts_s_one, 'end': ts_e_one}\n",
    "else:\n",
    "    x_ts = {'context': ts_context_padded, 'question': ts_question_padded}\n",
    "    y_ts = {'start': ts_s_one, 'end': ts_e_one}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "220a2a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutating model...\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 8.3277 - start_loss: 4.1462 - end_loss: 4.1815 - start_accuracy: 0.1213 - end_accuracy: 0.1334: 0s - loss: 8.5008 - start_loss: 4.2429 - end_loss: 4.2579 - start_accuracy: 0.1201 - end_accu\n",
      "[8.327722549438477, 4.146247863769531, 4.181472301483154, 0.12131715565919876, 0.13344886898994446]\n",
      "Preprocessing on datasets...\n",
      "Applying expand_contractions2, tokenization_spacy, remove_chars, split_alpha_num_sym and strip_text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\userl\\AppData\\Local\\Temp/ipykernel_2608/1903584515.py:6: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  df = pd.concat([train_df, val_df, test_df], 0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating predictions...\n",
      "Computing answers...\n",
      "Saving predictions as json...\n",
      "Computing F1 score, precision and recall...\n",
      "F1: 0.15346663335724842\t Precision: 0.175543284805333\t Recall: 0.20210205902976985\t\n"
     ]
    }
   ],
   "source": [
    "print(\"Evalutating model...\")\n",
    "evaluation = model.evaluate(x_ts, y_ts, batch_size=BATCH_SIZE)\n",
    "print(evaluation)\n",
    "\n",
    "\n",
    "df = pd.concat([train_df, val_df, test_df], 0, ignore_index=True)\n",
    "x = {**x_tr, **x_val, **x_ts}\n",
    "predictions = utils.computing_predictions(model, df, x, BATCH_SIZE)\n",
    "\n",
    "#predictions = utils.computing_predictions(model, train_df, val_df, test_df, x_tr, x_val, x_ts)\n",
    "\n",
    "print(\"Saving predictions as json...\")\n",
    "with open('predictions.json', 'w') as outfile:\n",
    "    json.dump(predictions, outfile)\n",
    "\n",
    "f1, precision, recall = utils.evaluate_model(model, MAX_CONTEXT_LENGTH, val_df1, x_val)\n",
    "print(f\"F1: {f1}\\t Precision: {precision}\\t Recall: {recall}\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c808c8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\userl\\AppData\\Local\\Temp/ipykernel_2608/2018551441.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.test import is_gpu_available\n",
    "is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a7950a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.config import list_physical_devices\n",
    "list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a106b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
