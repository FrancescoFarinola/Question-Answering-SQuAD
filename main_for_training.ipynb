{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219b4988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\userl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from tensorflow import one_hot\n",
    "import load_data\n",
    "import preprocess\n",
    "import utils\n",
    "import drqa_model\n",
    "import bidaf_model\n",
    "import our_model\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "\n",
    "from os.path import isfile\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json \n",
    "from settings import EMBEDDING_DIM, MODEL, EPOCHS, BATCH_SIZE, MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d3d4722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "(87599, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "dataframe = load_data.load_dataset()\n",
    "print(dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34761949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train and test set...\n",
      "(78001, 6) (9706, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting train and test set...\")\n",
    "train_df, test_df = load_data.split_test_set(dataframe)\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bd3ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train and validation set...\n",
      "(61009, 6) (16992, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting train and validation set...\")\n",
    "train_df, val_df = load_data.split_validation_set(train_df, rate=0.2)\n",
    "print(train_df.shape, val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9dbf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "Preprocessing validation data...\n"
     ]
    }
   ],
   "source": [
    "PREPROCESSING_PIPELINE1 = [preprocess.expand_contractions,\n",
    "                           preprocess.tokenization_spacy,\n",
    "                           preprocess.remove_chars,\n",
    "                           preprocess.split_alpha_num_sym,\n",
    "                           preprocess.spell_correction,\n",
    "                           preprocess.lemmatization,\n",
    "                           preprocess.lower,\n",
    "                           preprocess.strip_text]\n",
    "\n",
    "print(\"Preprocessing training data...\")\n",
    "train_df1 = train_df.copy()\n",
    "train_df1, train_tmp1 = preprocess.apply_preprocessing(train_df1, PREPROCESSING_PIPELINE1)\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "val_df1 = val_df.copy()\n",
    "val_df1, val_tmp1 = preprocess.apply_preprocessing(val_df1, PREPROCESSING_PIPELINE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32502566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load: True\n",
      "Loading matrices, tokenizers and dictionaries... \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# load already saved content or compute it from scratch\n",
    "load = (isfile(f\"{MODELS_DIR}/word_listing.csv\") and \n",
    "        isfile(f\"{MODELS_DIR}/word2idx.json\") and\n",
    "        isfile(f\"{MODELS_DIR}/idx2word.json\") and\n",
    "        isfile(f\"{MODELS_DIR}/tokenizer.json\") and\n",
    "        isfile(f\"{MODELS_DIR}/embedding_matrix.csv\"))\n",
    "print(\"load:\", load)\n",
    "\n",
    "if load:\n",
    "    print(\"Loading matrices, tokenizers and dictionaries... \")\n",
    "    #load pre-saved \n",
    "    df_word_listing = np.genfromtxt(f\"{MODELS_DIR}/word_listing.csv\", delimiter=',', encoding='utf-8', dtype='str')\n",
    "    \n",
    "    with open(f\"{MODELS_DIR}/word2idx.json\") as f:\n",
    "        df_word_to_idx = json.load(f)\n",
    "\n",
    "    with open(f\"{MODELS_DIR}/idx2word.json\") as f:\n",
    "        df_idx_to_word = json.load(f)\n",
    "\n",
    "    with open(f\"{MODELS_DIR}/tokenizer.json\") as f:\n",
    "        tokenizer_json = json.load(f)\n",
    "        df_tokenizer = tokenizer_from_json(tokenizer_json)\n",
    "\n",
    "    embedding_matrix = np.genfromtxt(f\"{MODELS_DIR}/embedding_matrix.csv\", delimiter=',')\n",
    "    print(\"Done\")\n",
    "          \n",
    "else:\n",
    "    #compute \n",
    "    print(\"Computing matrices, tokenizers and dictionaries... \")\n",
    "    embedding_matrix, df_word_listing, df_tokenizer, df_word_to_idx, df_idx_to_word = utils.get_embedding_matrix(train_df1, EMBEDDING_DIM)\n",
    "    \n",
    "    np.savetxt(f\"{MODELS_DIR}/embedding_matrix.csv\", embedding_matrix, delimiter=\",\")\n",
    "    np.savetxt(f\"{MODELS_DIR}/word_listing.csv\", df_word_listing, delimiter=\",\", fmt =\"%s\", encoding='utf-8')\n",
    "\n",
    "    with open(f\"{MODELS_DIR}/word2idx.json\", 'w') as f:\n",
    "        json.dump(df_word_to_idx, f)\n",
    "\n",
    "    with open(f\"{MODELS_DIR}/idx2word.json\", 'w') as f:\n",
    "        json.dump(df_idx_to_word, f)\n",
    "\n",
    "    tokenizer_json = df_tokenizer.to_json()\n",
    "    with io.open(f\"{MODELS_DIR}/tokenizer.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b10996e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx_to_word = dict(zip([int(k) for k in df_idx_to_word.keys()], df_idx_to_word.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c009329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding data...\n"
     ]
    }
   ],
   "source": [
    "MAX_CONTEXT_LENGTH, MAX_TEXT_LENGTH, MAX_QUESTION_LENGTH = utils.get_max_length(train_df1)\n",
    "\n",
    "#MAX_CONTEXT_LENGTH, MAX_TEXT_LENGTH, MAX_QUESTION_LENGTH = 662, 43, 40\n",
    "print(\"Padding data...\")\n",
    "tr_context_padded = utils.pad(train_df1.context, df_tokenizer, MAX_CONTEXT_LENGTH)\n",
    "tr_answer_padded = utils.pad(train_df1.text, df_tokenizer, MAX_TEXT_LENGTH)\n",
    "tr_question_padded = utils.pad(train_df1.question, df_tokenizer, MAX_QUESTION_LENGTH)\n",
    "\n",
    "val_context_padded = utils.pad(val_df1.context, df_tokenizer, MAX_CONTEXT_LENGTH)\n",
    "val_answer_padded = utils.pad(val_df1.text, df_tokenizer, MAX_TEXT_LENGTH)\n",
    "val_question_padded = utils.pad(val_df1.question, df_tokenizer, MAX_QUESTION_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76a9ec56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing start and end indices... \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing start and end indices... \")\n",
    "train_df1['s_idx'] = train_df.apply(\n",
    "    lambda x: len(preprocess.preprocessing(x.context[:x.answer_start], PREPROCESSING_PIPELINE1).split()), axis=1)\n",
    "train_df1['e_idx'] = train_df1.apply(lambda x: x.s_idx + len(x.text.split()) - 1, axis=1)\n",
    "\n",
    "val_df1['s_idx'] = val_df.apply(\n",
    "    lambda x: len(preprocess.preprocessing(x.context[:x.answer_start], PREPROCESSING_PIPELINE1).split()), axis=1)\n",
    "val_df1['e_idx'] = val_df1.apply(lambda x: x.s_idx + len(x.text.split()) - 1, axis=1)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e2b6a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>s_idx</th>\n",
       "      <th>e_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>beyoncé giselle knowles carter biːˈjɒnseɪ bee ...</td>\n",
       "      <td>269</td>\n",
       "      <td>in the late 1990 s</td>\n",
       "      <td>when did beyonce start becoming popular</td>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>42</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>beyoncé giselle knowles carter biːˈjɒnseɪ bee ...</td>\n",
       "      <td>207</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>what area did beyonce compete in when she wa g...</td>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>beyoncé giselle knowles carter biːˈjɒnseɪ bee ...</td>\n",
       "      <td>526</td>\n",
       "      <td>2003</td>\n",
       "      <td>when did beyonce leave destiny s child and bec...</td>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>beyoncé giselle knowles carter biːˈjɒnseɪ bee ...</td>\n",
       "      <td>166</td>\n",
       "      <td>houston texas</td>\n",
       "      <td>in what city and state did beyonce grow up</td>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>beyoncé giselle knowles carter biːˈjɒnseɪ bee ...</td>\n",
       "      <td>276</td>\n",
       "      <td>late 1990 s</td>\n",
       "      <td>in which decade did beyonce become famous</td>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>44</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16987</th>\n",
       "      <td>Geological_history_of_Earth</td>\n",
       "      <td>south america became linked to north america t...</td>\n",
       "      <td>682</td>\n",
       "      <td>the quaternary period</td>\n",
       "      <td>what period came after the pliocene</td>\n",
       "      <td>5732ae23cc179a14009dabfe</td>\n",
       "      <td>108</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16988</th>\n",
       "      <td>Geological_history_of_Earth</td>\n",
       "      <td>south america became linked to north america t...</td>\n",
       "      <td>158</td>\n",
       "      <td>marsupial fauna</td>\n",
       "      <td>the pliocene saw the end of what fauna in sout...</td>\n",
       "      <td>5732ae23cc179a14009dabfc</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16989</th>\n",
       "      <td>Geological_history_of_Earth</td>\n",
       "      <td>the last glacial period of the current ice age...</td>\n",
       "      <td>53</td>\n",
       "      <td>about 10000 year ago</td>\n",
       "      <td>how long ago did the last glacial period end</td>\n",
       "      <td>5732aeedcc179a14009dac04</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16990</th>\n",
       "      <td>Geological_history_of_Earth</td>\n",
       "      <td>the last glacial period of the current ice age...</td>\n",
       "      <td>124</td>\n",
       "      <td>35 metre 115 ft</td>\n",
       "      <td>by what height did sea level rise at the end o...</td>\n",
       "      <td>5732aeedcc179a14009dac05</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16991</th>\n",
       "      <td>Geological_history_of_Earth</td>\n",
       "      <td>the last glacial period of the current ice age...</td>\n",
       "      <td>1287</td>\n",
       "      <td>tyrrell sea</td>\n",
       "      <td>what sea did the hudson bay used to be a part of</td>\n",
       "      <td>5732aeedcc179a14009dac08</td>\n",
       "      <td>207</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16992 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "0                          Beyoncé   \n",
       "1                          Beyoncé   \n",
       "2                          Beyoncé   \n",
       "3                          Beyoncé   \n",
       "4                          Beyoncé   \n",
       "...                            ...   \n",
       "16987  Geological_history_of_Earth   \n",
       "16988  Geological_history_of_Earth   \n",
       "16989  Geological_history_of_Earth   \n",
       "16990  Geological_history_of_Earth   \n",
       "16991  Geological_history_of_Earth   \n",
       "\n",
       "                                                 context  answer_start  \\\n",
       "0      beyoncé giselle knowles carter biːˈjɒnseɪ bee ...           269   \n",
       "1      beyoncé giselle knowles carter biːˈjɒnseɪ bee ...           207   \n",
       "2      beyoncé giselle knowles carter biːˈjɒnseɪ bee ...           526   \n",
       "3      beyoncé giselle knowles carter biːˈjɒnseɪ bee ...           166   \n",
       "4      beyoncé giselle knowles carter biːˈjɒnseɪ bee ...           276   \n",
       "...                                                  ...           ...   \n",
       "16987  south america became linked to north america t...           682   \n",
       "16988  south america became linked to north america t...           158   \n",
       "16989  the last glacial period of the current ice age...            53   \n",
       "16990  the last glacial period of the current ice age...           124   \n",
       "16991  the last glacial period of the current ice age...          1287   \n",
       "\n",
       "                        text  \\\n",
       "0         in the late 1990 s   \n",
       "1        singing and dancing   \n",
       "2                       2003   \n",
       "3              houston texas   \n",
       "4                late 1990 s   \n",
       "...                      ...   \n",
       "16987  the quaternary period   \n",
       "16988        marsupial fauna   \n",
       "16989   about 10000 year ago   \n",
       "16990        35 metre 115 ft   \n",
       "16991            tyrrell sea   \n",
       "\n",
       "                                                question  \\\n",
       "0                when did beyonce start becoming popular   \n",
       "1      what area did beyonce compete in when she wa g...   \n",
       "2      when did beyonce leave destiny s child and bec...   \n",
       "3             in what city and state did beyonce grow up   \n",
       "4              in which decade did beyonce become famous   \n",
       "...                                                  ...   \n",
       "16987                what period came after the pliocene   \n",
       "16988  the pliocene saw the end of what fauna in sout...   \n",
       "16989       how long ago did the last glacial period end   \n",
       "16990  by what height did sea level rise at the end o...   \n",
       "16991   what sea did the hudson bay used to be a part of   \n",
       "\n",
       "                             id  s_idx  e_idx  \n",
       "0      56be85543aeaaa14008c9063     42     46  \n",
       "1      56be85543aeaaa14008c9065     31     33  \n",
       "2      56be85543aeaaa14008c9066     91     91  \n",
       "3      56bf6b0f3aeaaa14008c9601     25     26  \n",
       "4      56bf6b0f3aeaaa14008c9602     44     46  \n",
       "...                         ...    ...    ...  \n",
       "16987  5732ae23cc179a14009dabfe    108    110  \n",
       "16988  5732ae23cc179a14009dabfc     25     26  \n",
       "16989  5732aeedcc179a14009dac04     10     13  \n",
       "16990  5732aeedcc179a14009dac05     23     26  \n",
       "16991  5732aeedcc179a14009dac08    207    208  \n",
       "\n",
       "[16992 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beee73f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65239, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7bf7012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionaries for POS tags...\n",
      "Creating dictionaries for NER tags...\n",
      "Extracting features for Train Set\n",
      "Computing original exact match...\n",
      "Computing lowercase exact match...\n",
      "Computing lemmatized exact match...\n",
      "Computing normalized TF...\n",
      "Computing POS tags...\n",
      "Padding POS sequences...\n",
      "Computing NER tags...\n",
      "Padding NER sequences...\n",
      "Extracting features for Validation Set\n",
      "Computing original exact match...\n",
      "Computing lowercase exact match...\n",
      "Computing lemmatized exact match...\n",
      "Computing normalized TF...\n",
      "Computing POS tags...\n",
      "Padding POS sequences...\n",
      "Computing NER tags...\n",
      "Padding NER sequences...\n",
      "Computing character-level embeddings...\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "context (InputLayer)            [(None, 662)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question (InputLayer)           [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "p_encoding (Embedding)          (None, 662, 200)     13047800    context[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "char_p_encoding (Embedding)     (None, 662, 50)      3261950     context[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q_encoding (Embedding)          (None, 40, 200)      13047800    question[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "char_q_encoding (Embedding)     (None, 40, 50)       3261950     question[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat_p (Concatenate)          (None, 662, 250)     0           p_encoding[0][0]                 \n",
      "                                                                 char_p_encoding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concat_q (Concatenate)          (None, 40, 250)      0           q_encoding[0][0]                 \n",
      "                                                                 char_q_encoding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_p (Dense)                 (None, 662, 200)     50200       concat_p[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_q (Dense)                 (None, 40, 200)      50200       concat_q[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "biH (Bidirectional)             (None, 662, 200)     181200      dense_p[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "biU (Bidirectional)             (None, 40, 200)      181200      dense_q[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "S (SimilarityLayer)             (None, 662, 40)      600         biH[0][0]                        \n",
      "                                                                 biU[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "pos (InputLayer)                [(None, 662)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ner (InputLayer)                [(None, 662)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "C2Q (C2Q)                       (None, 662, 200)     0           biU[0][0]                        \n",
      "                                                                 S[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "Q2C (Q2C)                       (None, 662, 200)     0           biH[0][0]                        \n",
      "                                                                 S[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "em (InputLayer)                 [(None, 662, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_encoding (Embedding)        (None, 662, 54)      2916        pos[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "ner_encoding (Embedding)        (None, 662, 20)      400         ner[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf (InputLayer)                 [(None, 662, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "G (MergeG)                      (None, 662, 878)     0           biH[0][0]                        \n",
      "                                                                 C2Q[0][0]                        \n",
      "                                                                 Q2C[0][0]                        \n",
      "                                                                 em[0][0]                         \n",
      "                                                                 pos_encoding[0][0]               \n",
      "                                                                 ner_encoding[0][0]               \n",
      "                                                                 tf[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "M (Bidirectional)               (None, 662, 200)     588000      G[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "M2 (Bidirectional)              (None, 662, 200)     181200      M[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "GM (Concatenate)                (None, 662, 1078)    0           G[0][0]                          \n",
      "                                                                 M[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "GM2 (Concatenate)               (None, 662, 1078)    0           G[0][0]                          \n",
      "                                                                 M2[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "td_s (TimeDistributed)          (None, 662, 1)       1079        GM[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "td_e (TimeDistributed)          (None, 662, 1)       1079        GM2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze (TFOpLambd (None, 662)          0           td_s[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze_1 (TFOpLam (None, 662)          0           td_e[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "start_ (Softmax)                (None, 662)          0           tf.compat.v1.squeeze[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "end_ (Softmax)                  (None, 662)          0           tf.compat.v1.squeeze_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "prediction (Prediction)         (None, 662, 662)     0           start_[0][0]                     \n",
      "                                                                 end_[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "start (Lambda)                  (None, 662)          0           prediction[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "end (Lambda)                    (None, 662)          0           prediction[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 33,857,574\n",
      "Trainable params: 1,234,758\n",
      "Non-trainable params: 32,622,816\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "#save weights instead of saving entire model\n",
    "\n",
    "if MODEL == 'basemodel' or MODEL == None:\n",
    "    pass\n",
    "\n",
    "elif MODEL == 'drqa':\n",
    "    tag2idx, idx2tag = utils.create_pos_dicts()\n",
    "    ner2idx, idx2ner = utils.create_ner_dicts()\n",
    "\n",
    "    pos_embedding_matrix = to_categorical(list(idx2tag.keys()))\n",
    "    ner_embedding_matrix = to_categorical(list(idx2ner.keys()))\n",
    "\n",
    "    print(\"Extracting features for Train Set\")\n",
    "    train_em_input = utils.compute_exact_match(train_df1, MAX_CONTEXT_LENGTH)\n",
    "    train_tf_input = utils.compute_tf(train_df1, MAX_CONTEXT_LENGTH)\n",
    "    train_pos_input = utils.compute_pos(train_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    train_ner_input = utils.compute_ner(train_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    print(\"Extracting features for Validation Set\")\n",
    "    val_em_input = utils.compute_exact_match(val_df1, MAX_CONTEXT_LENGTH)\n",
    "    val_tf_input = utils.compute_tf(val_df1, MAX_CONTEXT_LENGTH)\n",
    "    val_pos_input = utils.compute_pos(val_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    val_ner_input = utils.compute_ner(val_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "\n",
    "    model = drqa_model.build_model(MAX_QUESTION_LENGTH, MAX_CONTEXT_LENGTH, EMBEDDING_DIM, embedding_matrix,\n",
    "                pos_embedding_matrix, ner_embedding_matrix)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics='accuracy')\n",
    "    model.summary()\n",
    "    plot_model(model, rankdir='TB', show_shapes=True, show_dtype=True, to_file=f\"{MODELS_DIR}/drqa.png\")\n",
    "\n",
    "    tr_s_one = one_hot(train_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    tr_e_one = one_hot(train_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_s_one = one_hot(val_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_e_one = one_hot(val_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    x_tr = {'context': tr_context_padded, 'question': tr_question_padded, 'pos': train_pos_input,\n",
    "            'ner': train_ner_input, 'em': train_em_input, 'tf': train_tf_input}\n",
    "    x_val = {'context': val_context_padded, 'question': val_question_padded, 'pos': val_pos_input,\n",
    "             'ner': val_ner_input, 'em': val_em_input, 'tf': val_tf_input}\n",
    "\n",
    "    y_tr = {'start': tr_s_one, 'end': tr_e_one}\n",
    "    y_val = {'start': val_s_one, 'end': val_e_one}\n",
    "\n",
    "    mycb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[mycb])\n",
    "    model.save_weights(f\"{MODELS_DIR}/drqa_weights.h5\")\n",
    "\n",
    "elif MODEL == \"bidaf\":\n",
    "\n",
    "    char_embedding_matrix = utils.get_char_embeddings(df_word_listing, df_word_to_idx)\n",
    "\n",
    "    model = bidaf_model.build_model(MAX_QUESTION_LENGTH, MAX_CONTEXT_LENGTH, EMBEDDING_DIM, embedding_matrix, char_embedding_matrix)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics='accuracy')\n",
    "    model.summary()\n",
    "    plot_model(model, rankdir='TB', show_shapes=True, show_dtype=True, to_file=f\"{MODELS_DIR}/bidaf.png\")\n",
    "\n",
    "    tr_s_one = one_hot(train_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    tr_e_one = one_hot(train_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_s_one = one_hot(val_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_e_one = one_hot(val_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    x_tr = {'context': tr_context_padded, 'question': tr_question_padded}\n",
    "    x_val = {'context': val_context_padded, 'question': val_question_padded}\n",
    "\n",
    "    y_tr = {'start': tr_s_one, 'end': tr_e_one}\n",
    "    y_val = {'start': val_s_one, 'end': val_e_one}\n",
    "\n",
    "    mycb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[mycb])\n",
    "    model.save(f\"{MODELS_DIR}/bidaf_weights.h5\")\n",
    "\n",
    "elif MODEL == \"our_model\":\n",
    "    tag2idx, idx2tag = utils.create_pos_dicts()\n",
    "    ner2idx, idx2ner = utils.create_ner_dicts()\n",
    "\n",
    "    pos_embedding_matrix = to_categorical(list(idx2tag.keys()))\n",
    "    ner_embedding_matrix = to_categorical(list(idx2ner.keys()))\n",
    "\n",
    "    print(\"Extracting features for Train Set\")\n",
    "    train_em_input = utils.compute_exact_match(train_df1, MAX_CONTEXT_LENGTH)\n",
    "    train_tf_input = utils.compute_tf(train_df1, MAX_CONTEXT_LENGTH)\n",
    "    train_pos_input = utils.compute_pos(train_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    train_ner_input = utils.compute_ner(train_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    print(\"Extracting features for Validation Set\")\n",
    "    val_em_input = utils.compute_exact_match(val_df1, MAX_CONTEXT_LENGTH)\n",
    "    val_tf_input = utils.compute_tf(val_df1, MAX_CONTEXT_LENGTH)\n",
    "    val_pos_input = utils.compute_pos(val_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    val_ner_input = utils.compute_ner(val_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    if isfile(f\"{MODELS_DIR}/char_embedding_matrix.csv\"):\n",
    "        char_embedding_matrix = np.genfromtxt(f\"{MODELS_DIR}/char_embedding_matrix.csv\", delimiter=',')\n",
    "    else:\n",
    "        char_embedding_matrix = utils.get_char_embeddings(df_word_listing, df_word_to_idx)\n",
    "        np.savetxt(f\"{MODELS_DIR}/char_embedding_matrix.csv\", char_embedding_matrix, delimiter=\",\")\n",
    "\n",
    "    model = our_model.build_model(MAX_QUESTION_LENGTH, MAX_CONTEXT_LENGTH, EMBEDDING_DIM,\n",
    "                                  embedding_matrix, char_embedding_matrix, pos_embedding_matrix, ner_embedding_matrix)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics='accuracy')\n",
    "    model.summary()\n",
    "    plot_model(model, rankdir='TB', show_shapes=True, show_dtype=True, to_file=f\"{MODELS_DIR}/our_model.png\")\n",
    "\n",
    "    tr_s_one = one_hot(train_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    tr_e_one = one_hot(train_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_s_one = one_hot(val_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "    val_e_one = one_hot(val_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    x_tr = {'context': tr_context_padded, 'question': tr_question_padded, 'pos': train_pos_input,\n",
    "            'ner': train_ner_input, 'em': train_em_input, 'tf': train_tf_input}\n",
    "    x_val = {'context': val_context_padded, 'question': val_question_padded, 'pos': val_pos_input,\n",
    "             'ner': val_ner_input, 'em': val_em_input, 'tf': val_tf_input}\n",
    "\n",
    "    y_tr = {'start': tr_s_one, 'end': tr_e_one}\n",
    "    y_val = {'start': val_s_one, 'end': val_e_one}\n",
    "\n",
    "    mycb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[mycb])\n",
    "    model.save_weights(f\"{MODELS_DIR}/our_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d1105ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7627/7627 [==============================] - 1634s 212ms/step - loss: 5.0953 - start_loss: 2.6665 - end_loss: 2.4287 - start_accuracy: 0.3420 - end_accuracy: 0.3691 - val_loss: 4.2744 - val_start_loss: 2.2259 - val_end_loss: 2.0485 - val_start_accuracy: 0.4207 - val_end_accuracy: 0.4506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x117a8269340>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=1, batch_size=8, callbacks=[mycb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26846af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(f\"{MODELS_DIR}/our_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6379c93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing original exact match...\n",
      "Computing lowercase exact match...\n",
      "Computing lemmatized exact match...\n",
      "Computing normalized TF...\n",
      "Computing POS tags...\n",
      "Padding POS sequences...\n",
      "Computing NER tags...\n",
      "Padding NER sequences...\n"
     ]
    }
   ],
   "source": [
    "test_df1 = test_df.copy()\n",
    "test_df1, test_tmp1 = preprocess.apply_preprocessing(test_df1, PREPROCESSING_PIPELINE1)\n",
    "\n",
    "ts_context_padded = utils.pad(test_df1.context, df_tokenizer, MAX_CONTEXT_LENGTH)\n",
    "ts_answer_padded = utils.pad(test_df1.text, df_tokenizer, MAX_TEXT_LENGTH)\n",
    "ts_question_padded = utils.pad(test_df1.question, df_tokenizer, MAX_QUESTION_LENGTH)\n",
    "\n",
    "test_df1['s_idx'] = test_df.apply(\n",
    "    lambda x: len(preprocess.preprocessing(x.context[:x.answer_start], PREPROCESSING_PIPELINE1).split()), axis=1)\n",
    "test_df1['e_idx'] = test_df1.apply(lambda x: x.s_idx + len(x.text.split()) - 1, axis=1)\n",
    "\n",
    "ts_s_one = one_hot(test_df1.s_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "ts_e_one = one_hot(test_df1.e_idx, depth=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "if MODEL == 'drqa' or MODEL == \"our_model\":\n",
    "\n",
    "    ts_em_input = utils.compute_exact_match(test_df1, MAX_CONTEXT_LENGTH)\n",
    "    ts_tf_input = utils.compute_tf(test_df1, MAX_CONTEXT_LENGTH)\n",
    "    ts_pos_input = utils.compute_pos(test_df1, tag2idx, MAX_CONTEXT_LENGTH)\n",
    "    ts_ner_input = utils.compute_ner(test_df1, ner2idx, MAX_CONTEXT_LENGTH)\n",
    "\n",
    "    x_ts = {'context': ts_context_padded, 'question': ts_question_padded, 'pos': ts_pos_input,\n",
    "            'ner': ts_ner_input, 'em': ts_em_input, 'tf': ts_tf_input}\n",
    "    y_ts = {'start': ts_s_one, 'end': ts_e_one}\n",
    "else:\n",
    "    x_ts = {'context': ts_context_padded, 'question': ts_question_padded}\n",
    "    y_ts = {'start': ts_s_one, 'end': ts_e_one}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "220a2a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutating model...\n",
      "1214/1214 [==============================] - 78s 64ms/step - loss: 4.9405 - start_loss: 2.5157 - end_loss: 2.4247 - start_accuracy: 0.4019 - end_accuracy: 0.4078 2s - loss: 4.9568 - start_loss: 2.5251 - end_loss: 2.4317 - start_\n",
      "[4.940453052520752, 2.5157387256622314, 2.4247169494628906, 0.40191635489463806, 0.4077889919281006]\n",
      "Preprocessing on datasets...\n",
      "Applying expand_contractions2, tokenization_spacy, remove_chars, split_alpha_num_sym and strip_text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\userl\\AppData\\Local\\Temp/ipykernel_10632/1903584515.py:6: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  df = pd.concat([train_df, val_df, test_df], 0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating predictions...\n",
      "Computing answers...\n",
      "Saving predictions as json...\n",
      "Computing F1 score, precision and recall...\n",
      "F1: 0.49550397738426266\t Precision: 0.530159225974904\t Recall: 0.5191881640497794\t\n"
     ]
    }
   ],
   "source": [
    "print(\"Evalutating model...\")\n",
    "evaluation = model.evaluate(x_ts, y_ts, batch_size=BATCH_SIZE)\n",
    "print(evaluation)\n",
    "\n",
    "\n",
    "df = pd.concat([train_df, val_df, test_df], 0, ignore_index=True)\n",
    "x = {**x_tr, **x_val, **x_ts}\n",
    "predictions = utils.computing_predictions(model, df, x, BATCH_SIZE)\n",
    "\n",
    "#predictions = utils.computing_predictions(model, train_df, val_df, test_df, x_tr, x_val, x_ts)\n",
    "\n",
    "print(\"Saving predictions as json...\")\n",
    "with open('predictions.json', 'w') as outfile:\n",
    "    json.dump(predictions, outfile)\n",
    "\n",
    "f1, precision, recall = utils.evaluate_model(model, MAX_CONTEXT_LENGTH, val_df1, x_val)\n",
    "print(f\"F1: {f1}\\t Precision: {precision}\\t Recall: {recall}\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69bd2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
